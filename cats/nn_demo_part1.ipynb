{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Neural Networks - Numpy Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author(s):** kozyr@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show how to train a very simple neural network from scratch (we're using nothing but ```numpy```!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identical to ```keras``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set up the data and network:\n",
    "n_outputs = 5  # We're attempting to learn XOR in this example, so our inputs and outputs will be the same.\n",
    "n_hidden_units = 10  # We'll use a single hidden layer with this number of hidden units in it.\n",
    "n_obs = 500  # How many observations of the XOR input to output vector will we use for learning?\n",
    "\n",
    "# How quickly do we want to update our weights?\n",
    "learning_rate = 0.1\n",
    "\n",
    "# How many times will we try to use each observation to improve the weights?\n",
    "epochs = 10  # Think of this as iterations if you like.\n",
    "\n",
    "# Set random seed so that the exercise works out the same way for everyone:\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some data to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identical to ```keras``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the inputs:\n",
    "training_vectors = np.random.binomial(1, 0.5, (n_obs, n_outputs))\n",
    "# Each row is a binary vector to learn from.\n",
    "print('One instance with ' + str(n_outputs) + ' features: ' + str(training_vectors[0]))\n",
    "\n",
    "# Create the correct XOR outputs (t is for target):\n",
    "xor_training_vectors = training_vectors ^ 1  # This is just XOR, everything is deterministic.\n",
    "print('Correct label (simply XOR):    ' + str(xor_training_vectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select activation and loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only in  ```numpy``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an activation function and its derivative:\n",
    "def activ(x):\n",
    "  # We'll use a sigmoid function:\n",
    "  return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def activ_prime(x):\n",
    "  # Derivative of the sigmoid function:\n",
    "  # d/dx 1 / (1 + exp(-x)) = -(-exp(-x)) * (1 + exp(-x)) ^ (-2)\n",
    "  return  np.exp(-x) / ((1.0 + np.exp(-x)) ** 2)\n",
    "  \n",
    "# Define a loss function and its derivative wrt predictions:\n",
    "def loss(prediction, truth):\n",
    "  # We'll choose cross entropy loss for this demo.\n",
    "  return -np.mean(truth * np.log(prediction) + (1 - truth) * np.log(1 - prediction))\n",
    "\n",
    "def loss_prime(prediction, truth):\n",
    "  # Derivative (elementwise) of cross entropy loss wrt prediction.\n",
    "  # d/dy (-t log(y) - (1-t)log(1-y)) = -t/y + (1-t)/(1-y) = -(t-ty-y+ty) = y - t\n",
    "  return prediction - truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only in  ```numpy``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest way to initialize is to choose weights uniformly at random between -1 and 1:\n",
    "weights1 = np.random.uniform(low=-1, high=1, size=(n_outputs, n_hidden_units))\n",
    "weights2 = np.random.uniform(low=-1, high=1, size=(n_hidden_units, n_outputs))\n",
    "# Note: there are much better ways to initialize weights, but our goal is simplicity here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only in  ```numpy``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, w1, w2):\n",
    "  \"\"\"Implements forward propagation.\n",
    "  \n",
    "  Args:\n",
    "    x: the input vector.\n",
    "    w1: first set of weights mapping the input to layer 1.\n",
    "    w2: second set of weights mapping layer 1 to layer 2.\n",
    "\n",
    "  Returns:\n",
    "    u1: unactivated unit values from layer 1 in forward prop\n",
    "    u2: unactivated unit values from layer 2 in forward prop\n",
    "    a1: activated unit values from layer 1 in forward prop          \n",
    "    a2: activated unit values from layer 2 in forward prop\n",
    "    lab: the output label\n",
    "  \"\"\"\n",
    "  u1 = np.dot(x, w1)  # u for unactivated weighted sum unit (other authors might prefer to call it z)\n",
    "  a1 = activ(u1)  # a for activated unit\n",
    "  u2 = np.dot(a1, w2)\n",
    "  a2 = activ(u2)\n",
    "  # Let's output predicted labels too, but converting continuous a2 to binary:\n",
    "  lab = (a2 > 0.5).astype(int)\n",
    "  return u1, u2, a1, a2, lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only in  ```numpy``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, t, u1, u2, a1, a2, w1, w2):\n",
    "  \"\"\"Implements backward propagation.\n",
    "  \n",
    "  Args:\n",
    "    x: the input vector\n",
    "    t: the desired output vector.\n",
    "    u1: unactivated unit values from layer 1 in forward prop\n",
    "    u2: unactivated unit values from layer 2 in forward prop\n",
    "    a1: activated unit values from layer 1 in forward prop          \n",
    "    a2: activated unit values from layer 2 in forward prop\n",
    "    w1: first set of weights mapping the input to layer 1.\n",
    "    w2: second set of weights mapping layer 1 to layer 2.\n",
    "  Returns: \n",
    "    d1: gradients for weights w1, used for updating w1\n",
    "    d2: gradients for weights w2, used for updating w2\n",
    "  \"\"\"\n",
    "  e2 = loss_prime(a2, t)  # e is for error; this is the \"error\" effect in the final layer\n",
    "  d2 = np.outer(a1, e2)  # d is for delta; this is the gradient value for updating weights w2\n",
    "  e1 = np.dot(w2, e2) * activ_prime(u1)  # e is for error\n",
    "  d1 = np.outer(x, e1)  # d is for delta; this is the gradient update for the first set of weights w1\n",
    "  return d1, d2  # We only need the updates outputted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only in  ```numpy``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "for epoch in range(epochs):\n",
    "  loss_tracker = []\n",
    "\n",
    "  for i in range(training_vectors.shape[0]):\n",
    "    # Input one obs at a time to become x = binary_vectors[i] (inputs) and t = xor_vectors[i] (targets)\n",
    "    # Forward propagation:\n",
    "    u1, u2, a1, a2, labels = forward_prop(training_vectors[i], weights1, weights2)\n",
    "    # Backward propagation:\n",
    "    d1, d2 = back_prop(training_vectors[i], xor_training_vectors[i],\n",
    "                       u1, u2, a1, a2, weights1, weights2)\n",
    "    # Update the weights:\n",
    "    weights1 -= learning_rate * d1\n",
    "    weights2 -= learning_rate * d2\n",
    "    loss_tracker.append(loss(prediction=a2, truth=xor_training_vectors[i]))\n",
    "\n",
    "  print 'Epoch: %d, Average Loss: %.8f' % (epoch+1, np.mean(loss_tracker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost identical to  ```keras``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance to screen:\n",
    "def get_performance(n_valid, w1, w2):\n",
    "  \"\"\"Computes performance and prints it to screen.\n",
    "  \n",
    "  Args:\n",
    "    n_valid: number of validation instances we'd like to simulate.\n",
    "    w1: first set of weights mapping the input to layer 1.\n",
    "    w2: second set of weights mapping layer 1 to layer 2.\n",
    "          \n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  flawless_tracker = []\n",
    "  validation_vectors = np.random.binomial(1, 0.5, (n_valid, n_outputs))\n",
    "  xor_validation_vectors = validation_vectors ^ 1\n",
    "\n",
    "  for i in range(n_valid):\n",
    "    u1, u2, a1, a2, labels = forward_prop(validation_vectors[i], w1, w2)\n",
    "    if i < 3:\n",
    "      print('********')\n",
    "      print('Challenge ' + str(i + 1) + ': ' + str(validation_vectors[i]))\n",
    "      print('Predicted ' + str(i + 1) + ': ' + str(labels))\n",
    "      print('Correct   ' + str(i + 1) + ': ' + str(xor_validation_vectors[i]))\n",
    "    instance_score = (np.array_equal(labels, xor_validation_vectors[i]))\n",
    "    flawless_tracker.append(instance_score)\n",
    "    \n",
    "  print('\\nProportion of flawless instances on ' + str(n_valid) +\n",
    "        ' new examples: ' + str(round(100*np.mean(flawless_tracker),0)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_performance(5000, weights1, weights2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
