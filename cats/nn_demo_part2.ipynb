{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Neural Networks - Keras Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author(s):** ronbodkin@google.com, kozyr@google.com, bfoo@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show how to train a very simple neural network from scratch (but let's upgrade from ```numpy``` to ```keras```). Keras is a higher-level API that makes TensorFlow easier to work work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identical to ```numpy``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set up the data and network:\n",
    "n_outputs = 5  # We're attempting to learn XOR in this example, so our inputs and outputs will be the same.\n",
    "n_hidden_units = 10  # We'll use a single hidden layer with this number of hidden units in it.\n",
    "n_obs = 500  # How many observations of the XOR input to output vector will we use for learning?\n",
    "\n",
    "# How quickly do we want to update our weights?\n",
    "learning_rate = 0.1\n",
    "\n",
    "# How many times will we try to use each observation to improve the weights?\n",
    "epochs = 10  # Think of this as iterations if you like.\n",
    "\n",
    "# Set random seed so that the exercise works out the same way for everyone:\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only in  ```keras``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Which version of TensorFlow are we using?\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add keras to runtime\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keras and basic types of NN layers we will use\n",
    "# Keras is a higher-level API for neural networks that works with TensorFlow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import keras.utils as np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some data to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identical to ```numpy``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the inputs:\n",
    "training_vectors = np.random.binomial(1, 0.5, (n_obs, n_outputs))\n",
    "# Each row is a binary vector to learn from.\n",
    "print('One instance with ' + str(n_outputs) + ' features: ' + str(training_vectors[0]))\n",
    "\n",
    "# Create the correct XOR outputs (t is for target):\n",
    "xor_training_vectors = training_vectors ^ 1  # This is just XOR, everything is deterministic.\n",
    "print('Correct label (simply XOR):    ' + str(xor_training_vectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network directly\n",
    "\n",
    "There's no need to write the loss and activation functions from scratch or compute the their derivatives, or to write forward and backprop from scratch.  We'll just select them and ```keras``` will take care of it. Thanks, ```keras```!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only in  ```keras``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer model with ReLU for hidden layer, sigmoid for output layer\n",
    "# Uncomment below to try a 3 layer model with two hidden layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=n_hidden_units, input_dim=n_outputs))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dense(units=n_hidden_units))\n",
    "#model.add(Activation('sigmoid'))\n",
    "model.add(Dense(units=n_outputs))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to choose an optimizer. Let's use SGD:\n",
    "sgd = keras.optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model using cross-entropy loss with SGD optimizer:\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only in  ```keras``` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model:\n",
    "model.fit(training_vectors, xor_training_vectors, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost identical to ```numpy``` version\n",
    "\n",
    "The only difference relates to the use of `model.predict()` and `model.evaluate()`.  See `loss_and_metrics` and `predicted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance to screen:\n",
    "def get_performance(n_valid):\n",
    "  \"\"\"Computes performance and prints it to screen.\n",
    "  \n",
    "  Args:\n",
    "    n_valid: number of validation instances we'd like to simulate.\n",
    "          \n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  flawless_tracker = []\n",
    "  validation_vectors = np.random.binomial(1, 0.5, (n_valid, n_outputs))\n",
    "  xor_validation_vectors = validation_vectors ^ 1\n",
    "        \n",
    "  loss_and_metrics = model.evaluate(validation_vectors,\n",
    "                                    xor_validation_vectors, batch_size=n_valid)\n",
    "  print(loss_and_metrics)\n",
    "\n",
    "  for i in range(n_valid):\n",
    "    predicted = model.predict(np.reshape(validation_vectors[i], (1,-1)), 1)\n",
    "    labels = (predicted > 0.5).astype(int)[0,]\n",
    "    if i < 3:\n",
    "      print('********')\n",
    "      print('Challenge ' + str(i + 1) + ': ' + str(validation_vectors[i]))\n",
    "      print('Predicted ' + str(i + 1) + ': ' + str(labels))\n",
    "      print('Correct   ' + str(i + 1) + ': ' + str(xor_validation_vectors[i]))\n",
    "    instance_score = (np.array_equal(labels, xor_validation_vectors[i]))\n",
    "    flawless_tracker.append(instance_score)\n",
    "  \n",
    "  print('\\nProportion of flawless instances on ' + str(n_valid) +\n",
    "        ' new examples: ' + str(round(100*np.mean(flawless_tracker),0)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_performance(5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
